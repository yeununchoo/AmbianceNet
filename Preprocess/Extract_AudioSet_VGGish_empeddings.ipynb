{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c45656-2c42-4f2f-85f3-197fb103e8b5",
   "metadata": {},
   "source": [
    "# AudioSet VGGish Embeddings\n",
    "\n",
    "Rule of the Internet: if you want to do something, someone else has already done it before.\n",
    "\n",
    "We should reference this random guy at Google\n",
    "\n",
    "- https://groups.google.com/g/audioset-users/c/wyyH4MqaboM\n",
    "- https://colab.research.google.com/drive/1BeORlWolTKw3noASvW94OXXqcQZ8PZEQ#scrollTo=r7BmTVx1tn_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c151544-8862-4ba6-bd60-d2832905ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import csv\n",
    "# import collections\n",
    "# import re\n",
    "\n",
    "# from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017449a0-85d1-4b62-9dd1-11939aceb8be",
   "metadata": {},
   "source": [
    "Before you run the cells below, you need to go to the AudioSet website first, and then you need to manually download the tar.gz file.\n",
    "\n",
    "- https://research.google.com/audioset/download.html\n",
    "- storage.googleapis.com/us_audioset/youtube_corpus/v1/features/features.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e01e9a-7768-48b9-aeb2-7bf4969cba76",
   "metadata": {},
   "source": [
    "## Load TF record files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a139e8-e35a-4fd5-bd92-c25848dcb71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TF record files: 1444\n",
      "['unbal_train/--.tfrecord', 'unbal_train/-0.tfrecord', 'unbal_train/-1.tfrecord', 'unbal_train/-2.tfrecord', 'unbal_train/-3.tfrecord', 'unbal_train/-4.tfrecord', 'unbal_train/-5.tfrecord', 'unbal_train/-6.tfrecord', 'unbal_train/-7.tfrecord', 'unbal_train/-8.tfrecord', 'unbal_train/-9.tfrecord', 'unbal_train/-A.tfrecord', 'unbal_train/-B.tfrecord', 'unbal_train/-c.tfrecord', 'unbal_train/-d.tfrecord', 'unbal_train/-E.tfrecord', 'unbal_train/-F.tfrecord', 'unbal_train/-G.tfrecord', 'unbal_train/-h.tfrecord', 'unbal_train/-i.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "# tfrecord_directory = \"bal_train\"\n",
    "# tfrecord_directory = \"eval\"\n",
    "tfrecord_directory = \"unbal_train\"\n",
    "tfrecord_filenames = os.listdir(tfrecord_directory)\n",
    "tfrecord_filenames = [(tfrecord_directory + \"/\" + each_fname) \n",
    "                      for each_fname \n",
    "                      in tfrecord_filenames]\n",
    "\n",
    "print(f\"Number of TF record files: {len(tfrecord_filenames)}\")\n",
    "print(tfrecord_filenames[:20])\n",
    "\n",
    "# practice with a small number of files first\n",
    "# it is not possible to load the entire dataset in a Windows machine anyway\n",
    "\n",
    "# bal_train: all 1444 available on Windows --> 7970 observations\n",
    "# eval     : all 1444 available on Windows --> 7329 observations\n",
    "# unbal_train:   first 20 files on Windows --> 19502 observations\n",
    "tfrecord_filenames = tfrecord_filenames[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c79ace-ada6-464b-b1c9-f6275da25c21",
   "metadata": {},
   "source": [
    "This is the dataset before parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687f02cd-3e3e-4a11-98cd-c8a4f0d23e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a8ba16-4414-4e29-a11f-9b9872c97276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for i, example in enumerate(raw_dataset):\n",
    "#     if (i%100_000) == 0:\n",
    "#         print(i)\n",
    "    \n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210eb763-e058-42de-ab9c-a74b85dcf68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, raw_record in enumerate(raw_dataset.take(1)):\n",
    "#     example = tf.train.SequenceExample()\n",
    "#     example.ParseFromString(raw_record.numpy())\n",
    "#     print(example)\n",
    "#     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be768904-c266-4eb7-8b5d-f51b2b5cd56c",
   "metadata": {},
   "source": [
    "## Parse the TF record files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51530736-ffbe-461b-8351-b8bf0d721702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a description of the features.\n",
    "context = {\n",
    "    'end_time_seconds': tf.io.FixedLenFeature([], tf.float32, \n",
    "                                              default_value = 0.0),\n",
    "    'video_id': tf.io.FixedLenFeature([], tf.string, \n",
    "                                      default_value = ''),\n",
    "    'start_time_seconds': tf.io.FixedLenFeature([], tf.float32, \n",
    "                                                default_value = 0.0),\n",
    "    'labels': tf.io.VarLenFeature(tf.int64),\n",
    "}\n",
    "\n",
    "sequence = {\n",
    "    'audio_embedding': tf.io.FixedLenSequenceFeature([], tf.string, \n",
    "                                                     default_value = None,\n",
    "                                                     allow_missing = True)\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    return tf.io.parse_single_sequence_example(example_proto, \n",
    "                                               context_features = context, \n",
    "                                               sequence_features = sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1fc08fb-3c72-4d22-a0f4-17b70373fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset = raw_dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae5ea4-8ab8-436b-a31c-0062ede43583",
   "metadata": {},
   "source": [
    "## Extract the music embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f227a9c-9186-48db-986b-90b361e830d2",
   "metadata": {},
   "source": [
    "### Class code to mood name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e096895b-56d1-4d8d-90d1-76539cc7141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Happy music', 'Funny music', 'Sad music', 'Tender music', 'Exciting music', 'Angry music', 'Scary music']\n"
     ]
    }
   ],
   "source": [
    "with open('class_labels_indices.csv', encoding='utf-8') as class_map_csv:\n",
    "    class_names = [display_name for (class_index, mid, display_name) in csv.reader(class_map_csv)]\n",
    "    class_names = class_names[1:]  # Skip CSV header\n",
    "\n",
    "#class_names = np.array(class_names)\n",
    "print(class_names[276:283])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f4c0a-fc2e-46e3-95a9-3963bc2b3d4e",
   "metadata": {},
   "source": [
    "### Visualize a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a521ea-239f-4d9e-8609-8413a5aff1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embedding(music_class_label):\n",
    "    music_count = 0\n",
    "    \n",
    "    for i, example in enumerate(parsed_dataset):\n",
    "        if (i%100_000) == 0:\n",
    "            print(f\"i = {i}\")\n",
    "\n",
    "        context, sequence = example\n",
    "        labels = context['labels'].values.numpy()\n",
    "\n",
    "        if (music_class_label in labels):\n",
    "            raw_embedding = sequence['audio_embedding'].numpy()\n",
    "            embedding = tf.io.decode_raw(raw_embedding, tf.int8).numpy()\n",
    "\n",
    "            plt.title(str(class_names[labels]))\n",
    "            plt.imshow(embedding, \n",
    "                       cmap = 'BrBG')\n",
    "            plt.xlabel(\"VGGish Embedding\")\n",
    "            plt.ylabel(\"Seconds\")\n",
    "            plt.show()\n",
    "\n",
    "            music_count += 1\n",
    "        \n",
    "        # this will give us THREE, not tow, visualizations\n",
    "        # because Python counts from zero\n",
    "        if music_count > 2:\n",
    "            break\n",
    "\n",
    "    print(f\"i = {i}\")\n",
    "    print(f\"music_count = {music_count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e4eaf8a-7db6-4871-ae3f-22ca392ed898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for each_label in range(276, 283):\n",
    "#     visualize_embedding(each_label)\n",
    "#     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9276740b-2d3b-4892-ab44-edfccc8b0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(music_class_label):\n",
    "    music_contexts = []\n",
    "    music_embeddings = []\n",
    "\n",
    "    for i, example in enumerate(parsed_dataset):\n",
    "        context, sequence = example\n",
    "        labels = context['labels'].values.numpy()\n",
    "\n",
    "        if (music_class_label in labels):\n",
    "            raw_embedding = sequence['audio_embedding'].numpy()\n",
    "            embedding = tf.io.decode_raw(raw_embedding, tf.int8).numpy()\n",
    "\n",
    "            if embedding.shape != (10, 128):\n",
    "                # ideally, all audio clips are exactly 10 seconds long\n",
    "                # however, some are shorter than 10 seconds. like 7 seconds for example\n",
    "                # so we pad the embedding with zero values to make the shape uniform\n",
    "\n",
    "                # print(f\"embedding old shape = {embedding.shape}\")\n",
    "\n",
    "                zero_padding = np.zeros((10 - embedding.shape[0], 128), dtype = 'int8')\n",
    "                embedding = np.concatenate((embedding, zero_padding), axis = 0)\n",
    "\n",
    "                # print(f\"embedding new shape = {embedding.shape}\")\n",
    "\n",
    "            music_embeddings.append(embedding)\n",
    "\n",
    "            music_context = (context['video_id'].numpy(), \n",
    "                             context['start_time_seconds'].numpy(), \n",
    "                             context['end_time_seconds'].numpy())\n",
    "\n",
    "            music_contexts.append(music_context)\n",
    "\n",
    "        # it is a long process\n",
    "        # good go know how much of the for loop is completed\n",
    "        # if (i%1000) == 0:\n",
    "        #     print(f\"i = {i}\")\n",
    "        \n",
    "    # print(f\"i = {i}\")\n",
    "    music_embeddings = np.array(music_embeddings)\n",
    "    \n",
    "    return music_contexts, music_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf41573-bad6-40b2-b170-1646f6a7708e",
   "metadata": {},
   "source": [
    "### VAP mapping\n",
    "\n",
    "NRC Word-Emotion Association Lexicon\n",
    "\n",
    "Source: https://saifmohammad.com/WebPages/nrc-vad.html\n",
    "\n",
    "We can't include the lexicon itself, because the non-commercial research license forbids redistributing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1f09823-ee56-4ff3-a3e4-b6b7f4c241f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAP_mapping = {\n",
    "    276:(1,0.735,0.772),\n",
    "    277:(0.918,0.61,0.566),\n",
    "    278:(0.225,0.333,0.149),\n",
    "    279:(0.63,0.52,0.509),\n",
    "    280:(0.95,0.792,0.789),\n",
    "    281:(0.122,0.83,0.604),\n",
    "    282:(0.062,0.952,0.528),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc79735-d7d6-40d9-9743-db950aa1cb5e",
   "metadata": {},
   "source": [
    "## Put all extractions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fd3c3cb-956a-42eb-9a01-0061317b1081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 39.4 s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "music_context_pd_before_concat = []\n",
    "music_embeddings_before_concat = []\n",
    "\n",
    "for each_class in range(276, 283):\n",
    "    music_contexts, music_embeddings = extract_embedding(each_class)\n",
    "    \n",
    "    (music_youtube_ids, music_start_times, music_end_times) = tuple(zip(*music_contexts))\n",
    "\n",
    "    music_context_pd = pd.DataFrame(data = {\"youtube_id\": music_youtube_ids, \n",
    "                                            \"start_time\": music_start_times, \n",
    "                                            \"end_time\"  : music_end_times})\n",
    "    music_context_pd[\"mood\"] = each_class\n",
    "    \n",
    "    valence, arousal, dominance = VAP_mapping[each_class]\n",
    "    music_context_pd[\"valence\"] = valence\n",
    "    music_context_pd[\"arousal\"] = arousal\n",
    "    music_context_pd[\"dominance\"] = dominance\n",
    "    \n",
    "#     print(\"music embeddings\")\n",
    "#     print(music_embeddings.shape)\n",
    "#     print(music_embeddings.dtype)\n",
    "\n",
    "#     print(\"\")\n",
    "#     print(\"music contexts\")\n",
    "#     print(music_context_pd.shape)\n",
    "#     print(music_context_pd.dtypes)\n",
    "#     display(music_context_pd.head())\n",
    "    \n",
    "    music_context_pd_before_concat.append(music_context_pd)\n",
    "    music_embeddings_before_concat.append(music_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69338fd5-4654-42fd-b703-25c6c1aea4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_context_all_moods_pd = pd.concat(music_context_pd_before_concat, \n",
    "                                       axis = 0, \n",
    "                                       ignore_index = True)\n",
    "\n",
    "music_embbedings_all_moods = np.concatenate(music_embeddings_before_concat, \n",
    "                                            axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f14fe63c-3579-4931-b056-d3c8c1e3ad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music_embbedings_all_moods\n",
      "(261, 10, 128)\n",
      "int8\n",
      "\n",
      "music_context_all_moods_pd\n",
      "(261, 7)\n",
      "youtube_id     object\n",
      "start_time    float32\n",
      "end_time      float32\n",
      "mood            int64\n",
      "valence       float64\n",
      "arousal       float64\n",
      "dominance     float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>youtube_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>mood</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'--CZ-8vrQ1g'</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'-0Rr5IvwyEY'</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'-1s2ecnv7SA'</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'-1B50kBrQeY'</td>\n",
       "      <td>170.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'-1Z0N9uJX7Q'</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       youtube_id  start_time  end_time  mood  valence  arousal  dominance\n",
       "0  b'--CZ-8vrQ1g'        30.0      40.0   276      1.0    0.735      0.772\n",
       "1  b'-0Rr5IvwyEY'        30.0      40.0   276      1.0    0.735      0.772\n",
       "2  b'-1s2ecnv7SA'        30.0      40.0   276      1.0    0.735      0.772\n",
       "3  b'-1B50kBrQeY'       170.0     180.0   276      1.0    0.735      0.772\n",
       "4  b'-1Z0N9uJX7Q'        30.0      40.0   276      1.0    0.735      0.772"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>youtube_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>mood</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>b'-ArVnrmCYkk'</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>282</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>b'-Bv8Dkss3UY'</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>282</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>b'-cZ1rm8HShQ'</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>282</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>b'-cxOEFwwNuY'</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>282</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>b'-F8HkbJFdKw'</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>282</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         youtube_id  start_time  end_time  mood  valence  arousal  dominance\n",
       "256  b'-ArVnrmCYkk'        30.0      40.0   282    0.062    0.952      0.528\n",
       "257  b'-Bv8Dkss3UY'        30.0      40.0   282    0.062    0.952      0.528\n",
       "258  b'-cZ1rm8HShQ'        20.0      30.0   282    0.062    0.952      0.528\n",
       "259  b'-cxOEFwwNuY'        20.0      30.0   282    0.062    0.952      0.528\n",
       "260  b'-F8HkbJFdKw'        30.0      40.0   282    0.062    0.952      0.528"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"music_embbedings_all_moods\")\n",
    "print(music_embbedings_all_moods.shape)\n",
    "print(music_embbedings_all_moods.dtype)\n",
    "\n",
    "print(\"\")\n",
    "print(\"music_context_all_moods_pd\")\n",
    "print(music_context_all_moods_pd.shape)\n",
    "print(music_context_all_moods_pd.dtypes)\n",
    "display(music_context_all_moods_pd.head())\n",
    "display(music_context_all_moods_pd.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b4fce-8332-4e81-8764-0abf8f9486ab",
   "metadata": {},
   "source": [
    "## Export the extracted embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5949b66-bf84-470c-b0c6-5e9a16138df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_filename = f\"music_mood/{tfrecord_directory}_music_contexts.parquet\"\n",
    "embeddings_filename = f\"music_mood/{tfrecord_directory}_music_embeddings.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf8204e-a3b7-406b-88a6-579cd87b52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_context_all_moods_pd.to_parquet(context_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "736b2807-bb84-4d69-a789-27d84b998665",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embeddings_filename, \"wb\") as f:\n",
    "    np.save(f, \n",
    "            music_embbedings_all_moods, \n",
    "            allow_pickle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee13809-4b3f-4e64-aa51-0b84ef17e5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
